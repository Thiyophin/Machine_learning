{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddef03f",
   "metadata": {},
   "source": [
    "# Neural Networks for Handwritten Digit Recognition, Multiclass \n",
    "\n",
    "In this exercise, you will use a neural network to recognize the hand-written digits 0-9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e67e0",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages \n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](https://numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
    "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbb8f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 05:34:52.502932: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 05:34:52.999421: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-26 05:34:53.001980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 05:34:54.134254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6cfa2",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU Activation\n",
    "a new activation was introduced, the Rectified Linear Unit (ReLU). \n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d62e3",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"data_files/common_activation_functions.png\"     style=\" width:380px; padding: 10px 20px; \" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37c059",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"data_files/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.     \n",
    "The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? This enables multiple units to contribute to to the resulting function without interfering. This is examined more in the supporting optional lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e51d43",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Softmax Function\n",
    "A multiclass neural network generates N outputs. One output is selected as the predicted answer. In the output layer, a vector $\\mathbf{z}$ is generated by a linear function which is fed into a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will sum to 1. They can be interpreted as probabilities. The larger inputs to the softmax will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"data_files/C2_W2_NNSoftmax.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf0dbd",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$\n",
    "\n",
    "Where $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ and N is the number of feature/categories in the output layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11630804",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "Let's create a NumPy implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8f8e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL: my_softmax\n",
    "\n",
    "def my_softmax(z):  \n",
    "    \"\"\" Softmax converts a vector of values to a probability distribution.\n",
    "    Args:\n",
    "      z (ndarray (N,))  : input data, N features\n",
    "    Returns:\n",
    "      a (ndarray (N,))  : softmax of z\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE ### \n",
    "    ez = np.exp(z)\n",
    "    a = ez/np.sum(ez)\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return a\n",
    "\n",
    "def my_softmax_2(z):  \n",
    "    N = len(z)\n",
    "    b =  np.zeros(N)                  # initialize a to zeros \n",
    "    ez_sum =  0           # initialize sum to zero\n",
    "    for k in range(N):      # loop over number of outputs             \n",
    "        ez_sum +=  np.exp(z[k])    # sum exp(z[k]) to build the shared denominator      \n",
    "    for j in range(N):      # loop over number of outputs again                \n",
    "        b[j] =  np.exp(z[j]) / ez_sum           # divide each the exp of each output by the denominator   \n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e4f80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_softmax(z):         [0.0320586  0.08714432 0.23688282 0.64391426]\n",
      "tensorflow softmax(z): [0.0320586  0.08714432 0.23688282 0.64391426]\n",
      "my_softmax_2(z):         [0.0320586  0.08714432 0.23688282 0.64391426]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([1., 2., 3., 4.])\n",
    "a = my_softmax(z)\n",
    "b = my_softmax_2(z)\n",
    "atf = tf.nn.softmax(z)\n",
    "print(f\"my_softmax(z):         {a}\")\n",
    "print(f\"tensorflow softmax(z): {atf}\") \n",
    "print(f\"my_softmax_2(z):         {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8abfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
